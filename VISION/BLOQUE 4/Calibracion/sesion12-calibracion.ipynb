{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b3e8e3-2c0a-4c0a-80c7-24eb9fef8862",
   "metadata": {},
   "source": [
    "# Proceso de formación de imagen\n",
    "\n",
    "En esta práctica trataremos el proceso de formación de la imagen desde un punto de vista geométrico. Específicamente, analizaremos un modelo básico de cámara que nos permitirá plantear las relaciones matemáticas por las cuales un punto en el espacio tridimensional se proyecta sobre el plano del sensor de la cámara, generando la imagen de salida. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4898050-2141-410e-8ec5-4b719186ef08",
   "metadata": {},
   "source": [
    "## Modelo de cámara\n",
    "\n",
    "Supongamos que tenemos un objeto y un sensor donde queremos registrar la imagen del objeto. Se dice que el objeto está situado en el plano objeto y su proyección se registra en el plano imagen, que suele coincidir con el plano donde está situado el sensor. Si usamos una configuración donde el objeto simplemente se enfrenta al sensor, todos los puntos del objeto reflejan la luz a todos los píxeles del sensor, por tanto la imagen que registramos en el sensor es una mancha borrosa, como se puede ver en la siguiente figura:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/modelo_objeto_sensor3.png\" width=\"700\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Figura 1: Imagen de un objeto sobre un sensor.    \n",
    "</center></figcaption></figure>\n",
    "\n",
    "Si se sitúa una lámina opaca con un pequeño agujero (**pin-hole**) entre el objeto y el sensor, filtramos los rayos y hacemos que la gran mayoría queden bloqueados. Como consecuencia, los rayos que pasan a través del agujero inciden en puntos diferentes del sensor dependiendo de su origen en el objeto. En otras palabras, tenemos un proceso de formación de imagen, como se puede ver en la siguiente figura:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/modelo_pinhole3.png\" width=\"1100\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Figura 2: Modelo de cámara \"pin-hole\".    \n",
    "</center></figcaption></figure>\n",
    "\n",
    "Este modo de proceder tiene el inconveniente de que la cantidad luz que pasa por la rendija es muy reducida, y para que funcione se requiere que la imagen se forme en una cámara oscura, que no es otra cosa que una caja cerrada con un agujero, **pin-hole**. Este es el principio de funcionamiento de la cámara oscura, la primera versión de lo que hoy conocemos como cámara fotográfica. Si se modifica la distancia del plano imagen (posición del sensor), se consigue ampliar o reducir la escala de la imagen. Este es el principio básico del funcionamiento del **zoom**, como se puede ver en la siguiente imagen:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/modelo_pinhole_distancia.png\" width=\"700\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Figura 3: Efecto de la distancia al plano imagen.    \n",
    "</center></figcaption></figure>\n",
    "\n",
    "Una cámara basada en un **pin-hole** tiene una buena definición del objeto si se realiza un buen filtrado de los rayos que tienden a dispersar la luz. Es decir, cuando el agujero es más reducido la imagen está mejor definida, a costa de reducir la cantidad de luz que llega al plano imagen. Para resolver este problema, el agujero se sustituye por una lente, que precisamente hace converger los rayos que tienden a dispersar la información procedente del objeto sobre el punto correspondiente en el plano imagen. El uso de una lente intensifica mucho la cantidad de luz que llega al plano imagen, ya que muchos de los rayos que no entrarían por el agujero consiguen entran por la lente y son re-direccionados hasta el punto correspondiente del objeto en el plano imagen. Un ejemplo de cámara con lente (objetivo) se puede ver en la siguiente figura:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/modelo_lente.png\" width=\"700\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Figura 4: Modelo de cámara con lente.    \n",
    "</center></figcaption></figure>\n",
    "\n",
    "Cuando se trabaja con cámaras, el plano de formación de la imagen está situado en el sensor, detrás de la apertura (lente o **pin-hole**). En este plano la imagen del objeto está invertida y los cálculos son un poco más complicados. Un modelo equivalente consiste en situar el plano imagen en un **plano virtual** situado antes de la apertura y a la misma distancia, denominada **distancia focal**, de tal manera que la imagen en este plano no está invertida. La línea que conecta el centro del sensor con el centro del **pin-hole** (o el centro de la lente) se llama **eje óptico** de la cámara. Típicamente, el **eje óptico** es una recta normal a la superficie del sensor y al plano imagen. Un ejemplo de esta situación se muestra en la siguiente figura:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/modelo_plano_imagen_virtual.png\" width=\"700\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Figura 5: Plano imagen virtual.    \n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "En el proceso de formación de imagen, el objetivo fundamental consiste en establecer la relación entre las coordenadas de un punto en el objeto y las coordenadas del punto en el plano imagen formado por la cámara. Para ello usaremos el modelo más sencillo de cámara, que consiste en considerar la cámara como un **pin-hole**. En este modelo se considera el plano donde se forma la **imagen virtual** situado a la distancia **focal** del **pin-hole**. A continuación definimos los distintos sistemas de coordenadas que podemos utilizar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40ceab54-59d0-4558-b979-fa7418bcc8ad",
   "metadata": {},
   "source": [
    "## Sistema de coordenadas del \"mundo\"\n",
    "\n",
    "Sea un punto en el campo de visión de la cámara, $\\mathbf{P}$, el objetivo consiste en encontrar las coordenadas de los píxeles $(u, v)$ de su proyección sobre el plano imagen, que es donde está situado el sensor de la cámara. La relación entre estos dos puntos se puede definir en diferentes ejes de coordenadas. Supongamos que definimos un sistema de coordenadas con el origen en una esquina de una habitación, denominado $\\mathbf{\\{X_w, Y_w, Z_w\\}}$. Este sistema de coordenadas es ajeno al proceso de formación de imagen y se denomina sistema de coordenadas del \"mundo\". En la Figura 6 se muestra este sistema de coordenadas en color naranja. Utilizaremos letras en negrita ( por ejemplo, $\\mathbf{X_w}$ ) para mostrar el eje de coordenadas, y letras normales para mostrar las coordenadas concretas del punto ( por ejemplo, $X_w$ ). Consideremos un punto $\\mathbf{P}$ en la habitación, sus coordenadas en el sistema de coordenadas del mundo vienen dadas por $(X_w, Y_w, Z_w)$.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/world-coordinates-and-camera-coordinates-768x432.png\" width=\"800\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Figura 6: Localización de un punto del espacio, P, mediante el sistema de coordenadas mundial (en amarillo). Se define también la posición de la cámara con su propio sistema de coordenadas (en rojo).    \n",
    "</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4635523e-4c65-4b91-9545-cff022c4ccef",
   "metadata": {},
   "source": [
    "## Sistema de coordenadas de la cámara\n",
    "\n",
    "A continuación nos centramos en la cámara, situada en la posición definida en la figura 6. Teniendo en cuenta que la cámara es la que proyecta el punto $\\mathbf{P}$ para poder formar la imagen en el sensor, también estamos interesados en un sistema de coordenadas tridimensional situado en la cámara y en su relación con el sistema de coordenadas del mundo. En la figura 6 se representa el sistema de coordenadas de la cámara, con color rojo. Según el sistema de coordenadas del mundo, la cámara está situada en un punto arbitrario, definido por las coordenadas $(t_x, t_y, t_z)$. Por lo tanto, los dos sistemas de coordenadas están desplazados por el vector $\\mathbf{t} = (t_x, t_y, t_z)$. Por otro lado, la cámara puede estar orientada en una dirección arbitraria, es decir, puede estar rotada con respecto al sistema de coordenadas del mundo. La rotación en el espacio tridimensional se caracteriza mediante tres ángulos de rotación --**yaw**, **pitch**, and **roll**--. Como hemos visto en la sesión anterior, la rotación viene definida por una multiplicación con una matriz de dimensiones $3 \\times 3$, que depende de los tres ángulos de rotación. Teniendo en cuenta todo lo anterior, el sistema de coordenadas mundial y el de la cámara están relacionados mediante una matriz de rotación $\\mathbf{R}$ y un vector de traslación $\\mathbf{t} = (t_x, t_y, t_z)$. Por tanto, el punto $\\mathbf{P}$, que tiene coordenadas $(X_w, Y_w, Z_w)$ en el sistema de coordenadas mundiales tendrá valores de coordenadas $(X_c, Y_c, Z_c)$ en el sistema de coordenadas de la cámara. Los valores de las coordenadas en los dos sistemas están relacionados por la siguiente ecuación:\n",
    "\n",
    "$$\\begin{bmatrix} X_c\\\\ Y_c\\\\ Z_c \\end{bmatrix}= \\mathbf{R} \\begin{bmatrix} X_w\\\\ Y_w\\\\ Z_w \\end{bmatrix} + \\mathbf{t}. $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abe52caf",
   "metadata": {},
   "source": [
    "Para incluir los desplazamientos en la matriz usamos las coordenadas homogéneas. De este modo el vector de traslación de dimensiones $3 \\times 1$ se añade como una columna al final de la matriz de rotación de $3 \\times 3$, dando lugar a una matriz $\\mathbf{E}$ de $3 \\times 4$ llamada **matriz extrínseca**.\n",
    "\n",
    "$$\\begin{bmatrix} X_c\\\\ Y_c\\\\ Z_c \\end{bmatrix}= \\begin{bmatrix} \\mathbf{R} | \\mathbf{t} \\end{bmatrix} \\begin{bmatrix} X_w\\\\ Y_w\\\\ Z_w \\\\ 1 \\end{bmatrix} = \\mathbf{E} \\begin{bmatrix} X_w\\\\ Y_w\\\\ Z_w \\\\ 1 \\end{bmatrix}, \\tag{1} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c4a1886",
   "metadata": {},
   "source": [
    "donde la matriz extrínseca $\\mathbf{E}$ viene dada por:\n",
    "\n",
    "$$\\mathbf{E} = \\begin{bmatrix} \\mathbf{R} | \\mathbf{t} \\end{bmatrix}. $$\n",
    "\n",
    "\n",
    "En resumen, el sistema de coordenadas mundial y el sistema de coordenadas de la cámara están relacionados por la rotación y la traslación de la cámara. Estos seis parámetros, 3 para la rotación y 3 para la traslación, se denominan parámetros extrínsecos y conforman lo que se denomina la matriz extrínseca."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222a3b6-dd8c-4bdc-8614-864c3017dc45",
   "metadata": {},
   "source": [
    "## Sistema de coordenadas de la imagen\n",
    "\n",
    "Ya tenemos expresadas las coordenadas del punto $\\mathbf{P}$ en el sistema de coordenadas de la cámara, el siguiente paso consiste en proyectar dicho punto sobre el plano imagen. Para ello, en la figura 7 se muestra un esquema de la proyección, donde se define el punto $\\mathbf{P}$ con coordenadas en el sistema de la cámara $(X_c, Y_c, Z_c)$. Consideramos la cámara en su modelo más simple, el modelo **pin-hole**. La luz que parte del punto $\\mathbf{P}$ se proyecta en el plano imagen virtual, con coordenadas $(x, y)$, y situado a una distancia $f$. El centro óptico de la cámara, que es el **pin-hole**, es el punto por el que pasan todos los rayos y se representa mediante $O_c$. El eje óptico conecta el **pin-hole**, $O_c$, con el centro del plano imagen, que se denomina **punto principal** y que define el origen del sistema de coordenadas situado en el plano imagen $\\mathbf{\\{x, y\\}}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "394ad668-f0e5-420f-a9df-964eea3103f5",
   "metadata": {},
   "source": [
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/world-camera-image-coordinates.png\" width=\"800\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Figura 7: Proyección del punto $\\mathbf{P}$ sobre el plano de la imagen.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Utilizando geometría básica (triángulos semejantes) obtenemos la relación entre coordenadas:\n",
    "\n",
    "\\begin{align*} x &= f \\frac{X_c}{Z_c} \\\\ y &= f \\frac{Y_c}{Z_c} \\end{align*}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41c0e22a",
   "metadata": {},
   "source": [
    "\n",
    "Las ecuaciones anteriores pueden reescribirse en forma matricial con coordenadas homogéneas:\n",
    "\n",
    "\\begin{equation*} \\begin{bmatrix} x' \\\\ y' \\\\ z' \\end{bmatrix}= \n",
    "\\begin{bmatrix} f & 0 & 0\\\\0 & f & 0\\\\ 0 & 0 & 1 \\end{bmatrix}\n",
    "\\begin{bmatrix} X_c\\\\ Y_c\\\\ Z_c \\end{bmatrix} = \\begin{bmatrix} X_cf\\\\ Y_cf\\\\ Z_c \\end{bmatrix},\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d932341",
   "metadata": {},
   "source": [
    "donde $\\begin{bmatrix} x' & y' & z' \\end{bmatrix}^T$ son las coordenadas homogéneas del plano imagen y $\\begin{bmatrix} X_c & Y_c & Z_c \\end{bmatrix}^T$ son las coordenadas homogéneas del plano objeto. Es interesante notar que si pasamos al sistema de coordendas hetereogéneo, recuperamos las relaciones de partida:\n",
    "\n",
    "$$\\begin{bmatrix} X_cf\\\\ Y_cf\\\\ Z_c\\end{bmatrix} \\Rightarrow  \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} X_cf/Z_c \\\\ Y_cf/Z_c \\end{bmatrix}.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e04ffa22-c9f5-4340-ad8c-cf8a1292b5ca",
   "metadata": {},
   "source": [
    "La matriz $\\textbf{K}$ se denomina **matriz intrínseca** y contiene los parámetros intrínsecos de la cámara.\n",
    "\n",
    "\\begin{align*} \\mathbf{K} = \\begin{bmatrix} f & 0 & 0 \\\\ 0 & f & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85f9b485",
   "metadata": {},
   "source": [
    "En el modelo de **pin-hole**, la matriz $\\mathbf{K}$ da cuenta de la distancia focal como factor de proporcionalidad entre dos triángulos, que se traduce en el factor de proporcionalidad entre objeto e imagen. Sin embargo, en una cámara real los píxeles del sensor pueden no ser cuadrados, lo cual da lugar a dos distancias focales diferentes para cada dirección, $f_x$ y $f_y$.\n",
    "\n",
    "Por otro lado, típicamente, en el plano imagen se suele adoptar el origen de coordenadas en la esquina superior izquierda de la imagen, en lugar del centro de la imagen que define el origen de las coordenadas $\\mathbf{\\{x, y\\}}$. Respecto de este nuevo eje de coordenadas, el centro de la imagen (que hemos denominado punto principal) tiene coordenadas $(c_x, c_y)$, que es necesario considerar en la matriz como un **offset** que desplaza el origen de coordenadas a la esquina superior izquierda. En este caso, las coordenadas de la proyección son $(u, v)$ y las unidades suelen ser píxeles. Además, también puede haber una pequeña rotación entre los ejes del sensor respecto de los ejes en la cámara, que viene definida por $\\gamma$. Teniendo en cuenta todas las consideraciones anteriores, la matriz intrínseca de la cámara puede reescribirse como:\n",
    "\n",
    "\\begin{align*} \\mathbf{K} = \\begin{bmatrix} f_x & \\gamma & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1\\end{bmatrix} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd16cf9",
   "metadata": {},
   "source": [
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/camera-projection-3D-to-2D.png\" width=\"800\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Figura 8. Proyección de la cámara de 3D a 2D.\n",
    "</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eec52413-5633-4026-ae87-80d669548382",
   "metadata": {},
   "source": [
    "Finalmente, las coordenadas en el plano imagen (expresadas en píxeles) vienen dadas por $(u', v')$, que se obtienen a través de la **matriz intrínseca**:\n",
    "\n",
    "\\begin{align*} \n",
    "\\begin{bmatrix} u' \\\\ v' \\\\ w' \\end{bmatrix} = \\mathbf{K} \\begin{bmatrix} X_c\\\\ Y_c\\\\ Z_c \\end{bmatrix}=\n",
    "\\begin{bmatrix} f_x & \\gamma & c_x \\\\ 0 & f_y & c_y \\\\ \n",
    "0 & 0 & 1 \\end{bmatrix} \n",
    "\\begin{bmatrix} X_c\\\\ Y_c\\\\ Z_c \\end{bmatrix}, \n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "855e88a8",
   "metadata": {},
   "source": [
    "donde:\n",
    "- $f_x$, $f_y$ son las distancias focales en $x$ e $y$, que suelen ser las mismas.\n",
    "- $c_x$, $c_y$ son las coordenadas $x$ e $y$ del punto principal, que normalmente coincide con el centro de la imagen.\n",
    "- $\\gamma$ es la desviación entre los ejes del sensor y de la cámara, que suele ser cero.\n",
    "\n",
    "Por otro lado, se puede relacionar el punto $\\mathbf{P}$ en coordenadas mundiales, $(X_w, Y_w, Z_w)$, con su proyección en coordenadas de la imagen, $(u, v)$. Teniendo en cuenta la ecuación (1), podemos poner las coordenadas de la cámara en función de las coordenadas del mundo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f10e1bb4",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align*} \n",
    "\\begin{bmatrix} u' \\\\ v' \\\\ w' \\end{bmatrix} = \\mathbf{K} \\begin{bmatrix} X_c\\\\ Y_c\\\\ Z_c \\end{bmatrix} = \\mathbf{K} \\cdot \\begin{bmatrix} \\mathbf{R} | \\mathbf{t} \\end{bmatrix} \\begin{bmatrix} X_w\\\\ Y_w\\\\ Z_w \\\\ 1 \\end{bmatrix} =  \\mathbf{M} \\begin{bmatrix} X_w \\\\ Y_w \\\\ Z_w \\\\ 1 \\end{bmatrix}, \\tag{2}\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d438f266",
   "metadata": {},
   "source": [
    "\n",
    "donde $\\mathbf{M}$ es una matriz de proyección de $3 \\times 4$, que está compuesta por el producto matricial de la **matriz intrínseca**, $\\mathbf{K}$, y la **matriz extrínseca** $[\\mathbf{R} \\mid \\mathbf{t}]$, que a su vez es la combinación de la matriz de rotación $\\mathbf{R}$ y un vector de traslación $\\mathbf{t}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{M} &= \\overbrace{\\mathbf{K}}^\\text{Matriz intrínseca} \\cdot \\overbrace{[\\mathbf{R} \\mid  \\mathbf{t}]}^\\text{Matriz extrínseca}\n",
    "\\end{align*}\n",
    "\n",
    "Finalmente, pasando las coordenadas homogénas a coordenadas hetereogéneas obtenemos la posición en el plano imagen, en unidades de píxeles:\n",
    "\n",
    "\\begin{align*} u &= \\frac{u'}{w'}\\\\ v &= \\frac{v'}{w'} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a30aa2",
   "metadata": {},
   "source": [
    "\n",
    "Como se puede ver, hemos considerado relaciones lineales entre las coordenadas. Cuando consideramos una cámara real, hay que sustituir el modelo de **pin-hole** por una cámara real con un objetivo, el cual consta de una o varias lentes para formar la imagen. El objetivo de la cámara introduce distorsiones y aberraciones sobre la imagen. Estos efectos también se pueden considerar como parámetros intrínsecos, pero requieren de relaciones no lineales entre las coordenadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb28c8-ac45-4287-85d5-336b8001d85f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cámara virtual\n",
    "\n",
    "En esta sección vamos a visualizar los cambios registrados en el plano imagen de una cámara cuando modificamos los parámetros intrínsecos o extrínsecos. Si modificamos los controles `X, Y, Z`, generamos un desplazamiento de la cámara en el plano tridimensional en el sistema de coordenadas del mundo. También podemos rotar la cámara en el sistema de coordenadas del mundo, en sus tres ángulos, lo cual cambiará la perspectiva con la que se registra la imagen. Tanto los desplazamientos como la rotación de la cámara son parámetros extrínsecos. Los parámetros `K1, K2, P1, P2, K3` son parámetros que caracterizan la distorsión del objetivo de la cámara, estos parámetros corresponden a una generalización del modelo de **pin-hole** en una cámara realista. Finalmente los parámetros `focal, Sx, Sy` caracterizan la distancia focal, que es la distancia a la que se sitúa el plano imagen, y los tamaños aparentes de los píxeles del sensor en los ejes `x` e `y`. Ejecute la celda siguiente para visualizar el efecto sobre la imagen y la matriz extrínseca de la cámara cuando el programa ha terminado. Visualice el efecto de cada uno de los parámetros en el proceso de formación de imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4accc5ab-ab48-489f-b82d-050f2b299dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############## Camera Matrix ##################\n",
      "[[ 0.96592583 -0.25881905  0.          0.        ]\n",
      " [ 0.25881905  0.96592583  0.          0.        ]\n",
      " [ 0.          0.          1.         35.        ]]\n"
     ]
    }
   ],
   "source": [
    "## Cámara virtual\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from data_calibracion.VirtualCam.vcam import vcam,meshGen\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "WINDOW_NAME = \"output\"\n",
    "cv2.namedWindow(WINDOW_NAME,cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(WINDOW_NAME,700,700)\n",
    "# Creating the tracker bar for all the features\n",
    "cv2.createTrackbar(\"X\",WINDOW_NAME,500,1000,nothing)\n",
    "cv2.createTrackbar(\"Y\",WINDOW_NAME,500,1000,nothing)\n",
    "cv2.createTrackbar(\"Z\",WINDOW_NAME,0,1000,nothing)\n",
    "cv2.createTrackbar(\"alpha\",WINDOW_NAME,180,360,nothing)\n",
    "cv2.createTrackbar(\"beta\",WINDOW_NAME,180,360,nothing)\n",
    "cv2.createTrackbar(\"gama\",WINDOW_NAME,180,360,nothing)\n",
    "#cv2.createTrackbar(\"K1\",WINDOW_NAME,0,100000,nothing)\n",
    "#cv2.createTrackbar(\"K2\",WINDOW_NAME,0,100000,nothing)\n",
    "#cv2.createTrackbar(\"P1\",WINDOW_NAME,0,100000,nothing)\n",
    "#cv2.createTrackbar(\"P2\",WINDOW_NAME,0,100000,nothing)\n",
    "cv2.createTrackbar(\"focus\",WINDOW_NAME,600,1000,nothing)\n",
    "cv2.createTrackbar(\"Sx\",WINDOW_NAME,100,1000,nothing)\n",
    "cv2.createTrackbar(\"Sy\",WINDOW_NAME,100,1000,nothing)\n",
    "# ret,img = cap.read()\n",
    "img = cv2.imread(\"data_calibracion/VirtualCam/chess.png\")\n",
    "H,W = img.shape[:2]\n",
    "c1 = vcam(H=H,W=W)\n",
    "plane = meshGen(H,W)\n",
    "plane.Z = plane.X*0 + 1\n",
    "pts3d = plane.getPlane()\n",
    "\n",
    "while True:\n",
    "    img = cv2.imread(\"data_calibracion/VirtualCam/chess.png\")\n",
    "    X = -cv2.getTrackbarPos(\"X\",WINDOW_NAME) + 500\n",
    "    Y = -cv2.getTrackbarPos(\"Y\",WINDOW_NAME) + 500\n",
    "    Z = -cv2.getTrackbarPos(\"Z\",WINDOW_NAME)\n",
    "    alpha = cv2.getTrackbarPos(\"alpha\",WINDOW_NAME) - 180\n",
    "    beta = cv2.getTrackbarPos(\"beta\",WINDOW_NAME) - 180\n",
    "    gamma = -cv2.getTrackbarPos(\"gama\",WINDOW_NAME) - 180\n",
    "    c1.focus = cv2.getTrackbarPos(\"focus\",WINDOW_NAME) - 500\n",
    "    c1.sx = (cv2.getTrackbarPos(\"Sx\",WINDOW_NAME)+1)/100\n",
    "    c1.sy = (cv2.getTrackbarPos(\"Sy\",WINDOW_NAME)+1)/100\n",
    "    #k1 = cv2.getTrackbarPos(\"K1\",WINDOW_NAME)/100000\n",
    "    #k2 = cv2.getTrackbarPos(\"K2\",WINDOW_NAME)/100000\n",
    "    #p1 = cv2.getTrackbarPos(\"P1\",WINDOW_NAME)/100000\n",
    "    #p2 = cv2.getTrackbarPos(\"P2\",WINDOW_NAME)/100000\n",
    "    #c1.KpCoeff[0] = k1\n",
    "    #c1.KpCoeff[1] = k2\n",
    "    #c1.KpCoeff[2] = p1\n",
    "    #c1.KpCoeff[3] = p2\n",
    "\n",
    "    c1.set_tvec(X,Y,Z)\n",
    "    c1.set_rvec(alpha,beta,gamma)\n",
    "    pts2d = c1.project(pts3d)\n",
    "    map_x,map_y = c1.getMaps(pts2d)\n",
    "    output = cv2.remap(img,map_x,map_y,interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    M = c1.RT\n",
    "    cv2.imshow(\"output\",output)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\\n\\n############## Camera Matrix ##################\")\n",
    "print(M)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4d31752",
   "metadata": {},
   "source": [
    "Para cerrar la ventana pulse la tecla `q`. Si se no se cierra la ventana, ejecute la siguiente celda para cerrarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48863fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed45ff6b-daba-4aed-9bba-263bf4e09308",
   "metadata": {},
   "source": [
    "# Calibración de una cámara\n",
    "\n",
    "En visión, un buen número de aplicaciones tienen que ver con la metrología, la automatización industrial, la robótica y la vigilancia. En todas estas aplicaciones es esencial conocer los parámetros característicos de la cámara, para poder realizar medidas a partir de las imágenes. El proceso de estimación de dichos parámetros se denomina calibración, e implica el conocimiento de la relación precisa entre un punto en el espacio tridimensional del mundo real y su correspondiente proyección bidimensional en el plano imagen, en unidades de píxeles. Específicamente, la calibración es el proceso para la obtención de tres tipos de parámetros:\n",
    "\n",
    "1. Parámetros externos: como hemos visto definen la rotación y traslación de la cámara con respecto al sistema de coordenadas mundial.\n",
    "2. Parámetros internos del sistema cámara-lente. Por ejemplo, la distancia focal y la posición del centro óptico. \n",
    "3. Un conjunto de parámetros que determinan la distorsión introducida por la lente del objetivo. Corresponden a una extensión del modelo de **pin-hole** para obtener una cámara real con su objetivo. \n",
    "\n",
    "El proceso de calibración consiste en encontrar la matriz $\\mathbf{M}$ utilizando la asociación entre un conjunto de puntos conocidos en el espacio tridimensional $(X_w, Y_w, Z_w)$ y esos mismos puntos en la imagen adquirida por la cámara, $(u, v)$. Es decir, es necesario encontrar un conjunto de correspondencias entre puntos del plano objeto y el plano imagen de la cámara, para poder resolver el sistema de ecuaciones de la expresión (2) y obtener las matrices extrínseca e intrínseca. Un algoritmo de calibración de cámaras tiene las siguientes entradas y salidas:\n",
    "\n",
    "- Entradas: Una colección de imágenes adquiridas con la cámara de un patrón de calibración (tablero de ajedrez o similar), para poder definir los puntos cuyas correspondencias en el plano objeto e imagen son conocidas. Las imágenes se obtienen con diferentes posiciones de la cámara y bajo diferentes ángulos de visión.\n",
    "- Salidas:\n",
    "    - La matriz intrínseca de la cámara, solo depende de parámetros internos de la cámara.\n",
    "    - La matriz extrínseca, la cual contiene la rotación y la traslación para cada imagen de entrada, ya que cada imagen ha sido tomada desde diferentes posiciones y perspectivas. \n",
    "\n",
    "En **OpenCV** no se considera el parámetro $\\gamma$ en la matriz intrínseca de la cámara, así que dicha matriz queda:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{K} =\n",
    "\\begin{bmatrix}\n",
    "f_x & 0 & c_x \\\\ \n",
    "0 & f_y & c_y\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a07c3-671e-4f26-86cc-7b62bbab6ef0",
   "metadata": {},
   "source": [
    "## Métodos de calibración\n",
    "A continuación se describen los principales tipos de métodos de calibración de cámaras:\n",
    "\n",
    "1. Patrón de calibración: Cuando tenemos un control total sobre el proceso de toma de imágenes, la mejor manera de realizar la calibración es capturar varias imágenes de un patrón de dimensiones conocidas desde diferentes puntos de vista. A continuación, utilizaremos este procedimiento con un tablero de ajedrez de dimensiones conocidas. \n",
    "\n",
    "2. Referencias geométricas: A veces tenemos otras referencias geométricas en la escena, como líneas rectas y puntos conocidos que se pueden utilizar para la calibración.\n",
    "\n",
    "3. Técnicas basadas en **deep learning**: Cuando tenemos muy poco control sobre la configuración de la imagen (por ejemplo, tenemos una sola imagen de la escena), aún es posible obtener información de la calibración de la cámara utilizando estos métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845f91f-af3a-40a9-bf49-315fb4c4d79a",
   "metadata": {},
   "source": [
    "## Calibración mediante un patrón conocido\n",
    "\n",
    "El proceso de calibración mediante un patrón conocido se explica en la siguiente figura.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/camera-calibration-flowchart-768x682.png\" width=\"400\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Diagrama de flujo del proceso de calibración de una cámara.  \n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "A continuación analizamos los diferentes pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494f690-b0c3-4dc7-a876-46f2a7806fcb",
   "metadata": {},
   "source": [
    "### 1. Definición de las coordenadas del mundo\n",
    "\n",
    "Los puntos del objeto utilizados para la calibración son las esquinas de los cuadrados del tablero. Por simplicidad, el sistema de coordenadas del mundo lo situamos en el plano objeto, es decir, el plano del tablero. Los ejes $\\mathbf{X_w}$ y $\\mathbf{Y_w}$ están en el plano del tablero y el eje $\\mathbf{Z_w}$ es perpendicular. Por lo tanto, los puntos del tablero cumplen que $Z_w = 0$. Para identificar los puntos usamos un tablero con dimensiones conocidas. Además, como los puntos están igualmente espaciados, las coordenadas $(X_w, Y_w)$ de cada punto se definen fácilmente tomando un punto como referencia $(0, 0)$ y definiendo las restantes con respecto a ese punto. Las esquinas de los cuadrados del tablero de ajedrez son utilizadas para localizarlas en el plano imagen, ya que tienen gradientes pronunciados en las dos direcciones. Además, estas esquinas también están relacionadas por el hecho de que se encuentran en la intersección entre líneas bien definidas. Estas circunstancias se utilizan para localizar de forma robusta las esquinas de los cuadrados.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/checkerboard-pattern-detection2.jpg\" width=\"800\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Resultado después de representar las esquinas del tablero detectadas.  \n",
    "</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf173c-11e3-4012-bab4-21797bc00fd2",
   "metadata": {},
   "source": [
    "### 2. Adquisición de imágenes\n",
    "\n",
    "El siguiente paso consiste en la adquisición de diferentes imágenes del tablero desde diferentes puntos de vista. El tablero debe estar estar fijo y desplazamos la cámara. Otra opción consiste en mantener la cámara fija y desplazar el tablero de ajedrez con diferentes orientaciones. Las dos situaciones son similares desde el punto de vista matemático. Un ejemplo de adquisición se representa en la siguiente figura:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/calibration-patterns.gif\" width=\"800\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Imágenes utilizadas para calibrar la cámara.\n",
    "</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efdffe47-5a50-48df-8a16-644295aa81bb",
   "metadata": {},
   "source": [
    "### 3. Identificación\n",
    "En este paso debemos encontrar las coordenadas en el plano imagen de los puntos correspondientes a las esquinas del tablero, para cada una de las imágenes adquiridas. Para ello, `OpenCV` proporciona una función integrada llamada `findChessboardCorners` que busca el tablero y devuelve las coordenadas de cada una de las esquinas. El último paso suele ser un refinamiento de la posición de las esquinas. Para obtener buenos resultados es importante obtener la ubicación con un nivel de precisión sub-pixel. La función `cornerSubPix` de **OpenCV** toma la imagen y la ubicación de las esquinas y busca el mejor ajuste sub-pixel dentro de una vecindad. El algoritmo es iterativo y por lo tanto necesitamos especificar los criterios de terminación (por ejemplo, el número de iteraciones y/o la precisión). La función tiene como parámetros:\n",
    "\n",
    "```corners_new = cv2.cornerSubPix(image, corners, winSize, zeroZone, criteria),```\n",
    "\n",
    "donde,\n",
    "- `image`: es la imagen de entrada.\n",
    "- `corners`: coordenadas iniciales de las esquinas de entrada.\n",
    "- `corners_new`: coordenadas refinadas de la imagen de entrada.\n",
    "- `winSize`: las dimensiones de la semi-longitud del lado de la ventana de búsqueda.\n",
    "- `zeroZone`: la semi-anchura de una región muerta en la mitad de la zona de búsqueda sobre la que no se calcula la matriz de autocorrelación. La matriz de autocorrelación es necesaria para el cálculo de la posición y ajustando esta ventana se suelen evitar posibles singularidades en la matriz. El valor de `(-1,-1)` indica que no existe tal ventana.\n",
    "- `criteria`: criterios de terminación del proceso iterativo de refinamiento. El proceso de refinamiento de la posición de las esquinas se detiene después de `criteria.maxCount` iteraciones o cuando la posición de las esquinas se mueve menos que `criteria.epsilon` en alguna iteración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389be697-2e70-4bd5-9e39-7899e32ea50b",
   "metadata": {},
   "source": [
    "### 4. Calibración\n",
    "\n",
    "El paso final consiste en estimar la matriz de calibración mediante el método `calibrateCamera`, para ello tenemos que pasar los puntos objeto en coordenadas mundiales y sus ubicaciones en el plano imagen, en cada una de las imagenes adquiridas. La  sintaxis de esta función es:\n",
    "\n",
    "``` retval, cameraMatrix, distCoeffs, rvecs, tvecs = cv2.calibrateCamera(objectPoints, imagePoints, imageSize),```\n",
    "\n",
    "A continuación mostramos el código necesario para realizar la calibración de la cámara:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cecc6839-2562-4818-a6c2-c78d3d3fc0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera matrix : \n",
      "\n",
      "[[503.68477277   0.         313.67563698]\n",
      " [  0.         503.37989192 243.25575466]\n",
      " [  0.           0.           1.        ]]\n",
      "dist : \n",
      "\n",
      "[[ 2.08346324e-01 -4.68650267e-01  4.51079102e-04 -1.93373875e-03\n",
      "   2.37592401e-01]]\n",
      "rvecs : \n",
      "\n",
      "(array([[0.78743976],\n",
      "       [0.53294469],\n",
      "       [1.52233962]]), array([[1.51596791e-03],\n",
      "       [6.69656746e-02],\n",
      "       [1.56241940e+00]]), array([[-0.23343341],\n",
      "       [-0.04621596],\n",
      "       [ 1.522275  ]]), array([[-0.31060359],\n",
      "       [ 0.1515558 ],\n",
      "       [ 1.57720394]]), array([[-1.46828017e-03],\n",
      "       [ 4.76768777e-02],\n",
      "       [ 1.56724499e+00]]), array([[-0.69506538],\n",
      "       [-0.57631163],\n",
      "       [ 1.45019819]]), array([[-0.15838481],\n",
      "       [ 0.05212478],\n",
      "       [ 1.52017069]]), array([[-0.2151081 ],\n",
      "       [ 0.31910018],\n",
      "       [ 1.53296626]]), array([[0.45057599],\n",
      "       [0.54006295],\n",
      "       [1.38860638]]), array([[-0.23749973],\n",
      "       [ 0.22031738],\n",
      "       [ 1.55996673]]), array([[-0.40676506],\n",
      "       [-0.72194813],\n",
      "       [ 1.36334531]]), array([[0.42519317],\n",
      "       [0.68127945],\n",
      "       [1.34442524]]), array([[-0.44555026],\n",
      "       [-0.30730569],\n",
      "       [ 1.53807621]]), array([[-0.15347862],\n",
      "       [ 0.39951894],\n",
      "       [ 1.49239868]]), array([[0.28284572],\n",
      "       [0.58780705],\n",
      "       [1.3194868 ]]), array([[-0.33899421],\n",
      "       [-0.47377128],\n",
      "       [ 1.59483249]]), array([[0.33307297],\n",
      "       [0.55841704],\n",
      "       [1.39532836]]), array([[-9.09292859e-04],\n",
      "       [ 3.66898483e-02],\n",
      "       [ 1.56666819e+00]]), array([[ 0.11068043],\n",
      "       [-0.46502899],\n",
      "       [ 1.48272651]]), array([[-0.01540714],\n",
      "       [ 0.08681116],\n",
      "       [ 1.54169614]]), array([[0.61951873],\n",
      "       [0.08486798],\n",
      "       [1.49114099]]), array([[-0.40374551],\n",
      "       [-0.06016137],\n",
      "       [ 1.53168687]]), array([[0.15172276],\n",
      "       [0.57093073],\n",
      "       [1.41375183]]), array([[0.3139181 ],\n",
      "       [0.58736866],\n",
      "       [1.39910762]]), array([[-0.21575755],\n",
      "       [ 0.38055436],\n",
      "       [ 1.50528033]]), array([[-0.1929761 ],\n",
      "       [ 0.37179966],\n",
      "       [ 1.55313095]]), array([[1.02215382],\n",
      "       [0.41241127],\n",
      "       [1.47957469]]), array([[-0.12600935],\n",
      "       [ 0.26517559],\n",
      "       [ 1.52229255]]), array([[0.84715318],\n",
      "       [0.19610195],\n",
      "       [1.46485457]]), array([[-0.55132416],\n",
      "       [-0.99705698],\n",
      "       [ 1.23526952]]), array([[-0.29895082],\n",
      "       [ 0.18643046],\n",
      "       [ 1.57442937]]), array([[-0.41607396],\n",
      "       [-0.7647259 ],\n",
      "       [ 1.33850247]]))\n",
      "tvecs : \n",
      "\n",
      "(array([[ 1.53264336],\n",
      "       [-2.4044174 ],\n",
      "       [ 8.16064041]]), array([[ 4.19975796],\n",
      "       [-2.45106708],\n",
      "       [ 9.34149497]]), array([[ 5.73455513],\n",
      "       [-2.70716913],\n",
      "       [14.02946082]]), array([[ 5.48323977],\n",
      "       [-1.00919762],\n",
      "       [12.7790685 ]]), array([[ 3.95672819],\n",
      "       [-2.67422839],\n",
      "       [ 8.57511754]]), array([[ 2.15837972],\n",
      "       [-2.69161395],\n",
      "       [14.54502612]]), array([[ 3.00290713],\n",
      "       [-2.37996174],\n",
      "       [19.24801655]]), array([[ 3.15331369],\n",
      "       [-2.52597187],\n",
      "       [11.1719889 ]]), array([[ 3.05920062],\n",
      "       [-2.48136724],\n",
      "       [ 9.46742967]]), array([[ 2.7533763 ],\n",
      "       [-1.26097867],\n",
      "       [12.69840455]]), array([[ 3.45373763],\n",
      "       [-3.32563976],\n",
      "       [12.92869075]]), array([[ 0.68602451],\n",
      "       [-2.48343549],\n",
      "       [10.55202065]]), array([[ 5.43496492],\n",
      "       [-2.41530304],\n",
      "       [14.43833828]]), array([[ 3.49959138],\n",
      "       [-2.02204681],\n",
      "       [14.0114402 ]]), array([[ 1.91174365],\n",
      "       [-3.16265821],\n",
      "       [ 8.87645259]]), array([[ 5.0942735 ],\n",
      "       [-2.62523423],\n",
      "       [14.86684381]]), array([[ 2.78198101],\n",
      "       [-2.71047535],\n",
      "       [ 9.46752665]]), array([[ 3.74275493],\n",
      "       [-2.91015673],\n",
      "       [ 8.21848023]]), array([[ 4.82906643],\n",
      "       [-2.55201216],\n",
      "       [ 9.98282155]]), array([[ 3.71941416],\n",
      "       [-2.58185512],\n",
      "       [ 8.4887555 ]]), array([[ 2.84015628],\n",
      "       [-2.06069187],\n",
      "       [ 7.18834522]]), array([[ 2.21391313],\n",
      "       [-2.32755226],\n",
      "       [14.87011038]]), array([[ 1.67727173],\n",
      "       [-2.44980131],\n",
      "       [ 9.10099492]]), array([[ 2.56380073],\n",
      "       [-2.03560397],\n",
      "       [ 9.88163046]]), array([[ 3.58609877],\n",
      "       [-2.24346754],\n",
      "       [10.52411658]]), array([[ 4.59876129],\n",
      "       [-2.88581944],\n",
      "       [11.2694964 ]]), array([[ 2.16201438],\n",
      "       [-2.05542549],\n",
      "       [ 7.39413122]]), array([[ 3.4677332 ],\n",
      "       [-2.25213533],\n",
      "       [ 9.84036389]]), array([[ 1.07904219],\n",
      "       [-2.10327764],\n",
      "       [ 7.64270833]]), array([[ 0.94099621],\n",
      "       [-4.46958493],\n",
      "       [14.14987195]]), array([[ 5.11289273],\n",
      "       [-0.85351624],\n",
      "       [13.36840857]]), array([[ 2.927001  ],\n",
      "       [-3.8124096 ],\n",
      "       [13.08614645]]))\n"
     ]
    }
   ],
   "source": [
    "# procedimiento de calibración\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Defining the dimensions of checkerboard\n",
    "CHECKERBOARD = (6,9)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# Creating vector to store vectors of 3D points for each checkerboard image\n",
    "objpoints = []\n",
    "# Creating vector to store vectors of 2D points for each checkerboard image\n",
    "imgpoints = [] \n",
    "\n",
    "# Defining the world coordinates for 3D points\n",
    "objp = np.zeros((1, CHECKERBOARD[0]*CHECKERBOARD[1], 3), np.float32)\n",
    "objp[0,:,:2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\n",
    "prev_img_shape = None\n",
    "\n",
    "# Extracting path of individual image stored in a given directory\n",
    "images = glob.glob('data_calibracion/figs/calibracion/*.jpg')\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    #img = cv2.resize(img, None, fx=0.25, fy=0.25, interpolation = cv2.INTER_LINEAR)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    # Find the chess board corners\n",
    "    # If desired number of corners are found in the image then ret = true\n",
    "    ret, corners = cv2.findChessboardCorners(gray, CHECKERBOARD, cv2.CALIB_CB_ADAPTIVE_THRESH+\n",
    "    \tcv2.CALIB_CB_FAST_CHECK+cv2.CALIB_CB_NORMALIZE_IMAGE)\n",
    "    \"\"\"\n",
    "    If desired number of corner are detected,\n",
    "    we refine the pixel coordinates and display \n",
    "    them on the images of checker board\n",
    "    \"\"\"\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        # refining pixel coordinates for given 2d points.\n",
    "        corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
    "        imgpoints.append(corners2)\n",
    "       # Draw and display the corners\n",
    "        img = cv2.drawChessboardCorners(img, CHECKERBOARD, corners2,ret)\n",
    "    cv2.imshow('img',img)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "h,w = img.shape[:2]\n",
    "\"\"\"\n",
    "Performing camera calibration by \n",
    "passing the value of known 3D points (objpoints)\n",
    "and corresponding pixel coordinates of the \n",
    "detected corners (imgpoints)\n",
    "\"\"\"\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)\n",
    "\n",
    "print(\"Camera matrix : \\n\")\n",
    "print(mtx)\n",
    "print(\"dist : \\n\")\n",
    "print(dist)\n",
    "print(\"rvecs : \\n\")\n",
    "print(rvecs)\n",
    "print(\"tvecs : \\n\")\n",
    "print(tvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee68af1-90c5-4ace-aeb8-33e1d069c3f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Después del proceso de calibración obtenemos la matriz intrínseca de la cámara, los coeficientes  de distorsión de la lente y los vectores que describen las diferentes traslaciones y rotaciones de la cámara para la captura de las imágenes. Los vectores de traslación y rotación son los parámetros extrínsecos y nos permiten reconstruir el proceso de calibración respecto del sistema de coordenadas del mundo, según se puede ver en la siguiente figura, donde se han enumerado cada una de las posiciones de la cámara en el proceso de adquisición:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/param-extrinsic.gif\" width=\"500\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Los parámetros extrínsecos determinan la posición de la cámara.\n",
    "</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c9d61b-b8d3-41be-86bd-5d5770eaeb10",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distorsión de la lente\n",
    "\n",
    "Como hemos comentado, el modelo de **pin-hole** es un modelo simple que permite entender el proceso de formación de la imagen. En la práctica, el agujero de la cámara es sustituido por un objetivo. El objetivo contiene una o varias lentes y permite obtener una imagen más nítida y mejor iluminada. Como contraprestación, el uso de lentes genera ciertas distorsiones sobre la imagen registrada, entre las que se encuentran:\n",
    "\n",
    "- **Distorsión radial**: suele producirse porque los rayos de luz son sometidos a una desviación no uniforme. Los rayos se desvían de forma diferente cuando inciden cerca de los bordes de la lente comparado con la desviación cuando inciden cerca del centro de la lente. Debido a la distorsión radial, las líneas rectas en el plano objeto aparecen curvadas en el plano imagen. Un rayo de luz se desplaza radialmente hacia dentro o hacia fuera de su ubicación ideal antes de llegar al sensor de imagen. Hay dos tipos de efecto de distorsión radial: la distorsión de **barril** y la distorsión de **cojín**.\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/radial-distortions.png\" width=\"600\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Distorsiones radiales.  \n",
    "</center></figcaption>\n",
    "</figure>\n",
    "La distorsión radial se puede representar matemáticamente como:\n",
    "\n",
    "$$x_{distorted} = x( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\\\ y_{distorted} = y( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6)$$\n",
    "\n",
    "- **Distorsión tangencial**: suele producirse cuando el sensor de la cámara se encuentra inclinado con un ángulo con respecto al objetivo. De modo que la imagen parece estar inclinada y estirada.\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/tangential-and-radial-distortion-effect.jpg\" width=\"600\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center> Distorsión radial y tangencial a la izquierda y efecto de la distorsión tangencial a la derecha.  \n",
    "</center></figcaption>\n",
    "</figure>\n",
    "Según se puede ver en la figura, la distorsión tangencial es una desviación del ángulo de rotación respecto del centro de la lente, mientras que la distorsión radial es una desviación respecto del radio. La distorsión tangencial se representa matemáticamente como:\n",
    "\n",
    "$$x_{distorted} = x + [ 2p_1xy + p_2(r^2+2x^2)] \\\\ y_{distorted} = y + [ p_1(r^2+ 2y^2)+ 2p_2xy]$$\n",
    "\n",
    "La distorsión de la lente complica la realización de cualquier medida realizada con la cámara, por ello es necesario caracterizar matemáticamente las distorsiones. Los efectos de la distorsión son también parámetros intrínsecos de la cámara y, como hemos visto, se obtienen en el proceso de calibración. La distorsión queda caracterizada por los coeficientes $(k_1, k_2, p_1, p_2, k_3)$, los cuales permiten obtener las nuevas coordenadas mediante una transformación no lineal. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84428fbf-d048-4d36-a59c-9f102000a136",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compensación de la distorsión\n",
    "\n",
    "Una de las aplicaciones de la caracterización de la distorsión consiste utilizar los coeficientes para implementar otra distorsión en sentido contrario que pueda compensar la distorsión original. En la siguiente imagen se han utilizado los parámetros de calibración para eliminar la distorsión, como se puede ver en la imagen de la derecha.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<center><img src=\"data_calibracion/figs/geometric-calibration.jpg\" width=\"800\" alt=\" \"></center>\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Efecto de la corrección geométrica en la imagen distorsionada.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Para eliminar la distorsión de la imagen se requieren tres pasos.\n",
    "\n",
    "- Realizar la calibración de la cámara y obtener los parámetros intrínsecos, incluyendo los parámetros de distorsión de la cámara.\n",
    "- Al eliminar la distorsión de la imagen deformamos el plano de la imagen y suelen aparecer píxeles indefinidos en el contorno de la imagen. Estos píxeles no contienen información de la imagen y conviene eliminarlos. Para ello aplicamos el método `getOptimalNewCameraMatrix()`, el cual realiza un refinamiento de la matriz intrínseca de la cámara y ajusta el porcentaje de píxeles no validos en el contorno a través del parámetro `alpha`. El método devuelve las coordenadas de la ROI de la imagen, la cual puede ser utilizada para recortar la imagen y excluir estos puntos.\n",
    "- Utilizar la matriz refinada de la cámara para eliminar la distorsión la imagen.\n",
    "\n",
    "En la siguiente celda se presentan dos métodos para eliminar la distorsión de la primera imagen utilizada en la calibración. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6183689a-9aa9-4cb5-b924-25ed30ebb2f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the derived camera parameters to undistort the image\n",
    "img = cv2.imread(images[0])\n",
    "# Refining the camera matrix using parameters obtained by calibration\n",
    "newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))\n",
    "\n",
    "# Method 1 to undistort the image\n",
    "dst = cv2.undistort(img, mtx, dist, None, newcameramtx)\n",
    "# Method 2 to undistort the image\n",
    "mapx,mapy=cv2.initUndistortRectifyMap(mtx,dist,None,newcameramtx,(w,h),5)\n",
    "dst = cv2.remap(img,mapx,mapy,cv2.INTER_LINEAR)\n",
    "\n",
    "# Displaying the undistorted image\n",
    "cv2.imshow(\"distorted image\",img)\n",
    "cv2.imshow(\"undistorted image\",dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2b678-b8de-48f1-b21a-c51e435b61c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Para evaluar la exactitud de la calibración, se puede volver a proyectar la imagen compensada de distorsión para volver a obtener la versión distorsionada. Comparando la imagen original con la imagen re-proyectada obtenemos una medida del error cometido por el proceso de calibración y compensación de la distorsión. Este error se llama **error de re-proyección**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e336a0-03a3-4006-a012-2c94170eb81c",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "El proceso de formación de imagen en una cámara se puede obtener mediante un modelo sencillo de cámara denominado **pin-hole**. El proceso de formación de imagen se puede caracterizar mediante una correspondencia entre un punto en el campo de visión de una cámara, definido en un sistema de coordenadas del mundo, con el píxel correspondiente en el plano imagen, en el sistema de coordenadas del propio plano imagen. Esta correspondencia se establece en dos pasos. En primer lugar, las coordenadas mundiales del punto se transforman en las coordenadas de la cámara, utilizando la **Matriz Extrínseca**. Esta conversión requiere una rotación y una traslación entre los dos sistemas de coordenadas. El nuevo punto en el sistema de coordenadas de la cámara se proyecta sobre el plano imagen utilizando la **Matriz Intrínseca**, que consiste en la aplicación de los parámetros internos de la cámara: como la distancia focal, la posición del centro óptico, o el tamaño aparente de los píxeles. La calibración de una cámara implica la obtención de los parámetros intrínsecos y extrínsecos de la misma, para ello hemos mostrado un procedimiento por el cual se toman diferentes imágenes de un tablero de ajedrez con diferentes posiciones y orientaciones de la cámara. Todas las imágenes tienen puntos correspondientes que proceden de un objeto original con coordenadas conocidas en el sistema de coordenadas del mundo. Con la información de las asociaciones entre los puntos correspondientes de las diferentes imágenes se resuelve un sistema de ecuaciones que determina los parámetros de la cámara. Una vez está caracterizado el proceso de formación de imagen con sus distorsiones, se pueden utilizar los parámetros de la cámara para eliminar la distorsión de la misma. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fbac8df-3c9b-42f7-9adf-77014f47119e",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "- Richard Hartley y Andrew Zisserman, \"Multiple View Geometry in Computer Vision\".\n",
    "- J. Weng, P. Cohen, and M. Herniou. Camera calibration with distortion models and accuracy evaluation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(10):965–980, Oct. 1992.\n",
    "- Documentación de OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff83ed-ef2c-44b6-86e2-ff38cede0627",
   "metadata": {},
   "source": [
    "# Preguntas\n",
    "\n",
    "**La adquisición de las competencias asociadas a las prácticas requieren una explicación propia y rigurosa. Las copias y traducciones literales no demuestran la adquisición de las competencias, por tanto no se considerarán válidas (consultar la rúbrica).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e640f26",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "1. ¿Qué tipo de transformaciones relaciona el sistema de coordenadas del mundo con el sistema de coordenadas de la cámara? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f722d88-4086-4b08-b9f1-60b4bd9cf645",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638dedb-c493-47a6-86dc-d39104eaeb57",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "2. ¿De qué variables depende la matrix extrínseca? ¿Por qué es interesante plantear la transformación en coordenadas homogéneas?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca160003-99e6-4936-9061-7210fa9b9c4d",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159db87-ac59-4ffb-bb9f-010e773a42bd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "3. Describa el cometido de la matriz intrínseca de la cámara. ¿Qué dos sistemas de coordenadas relaciona? Describa brevemente el significado de los elementos de la matriz.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994e624-a9eb-415e-a4e5-1f77627326cf",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0affb86a-b17b-4287-8b26-1db96c6af23d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "4. Ejecute el código \"## Cámara virutal\". Comente el efecto que produce sobre la imagen cada uno de los parámetros configurables mediante las \"Trackbars\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f9ca6-e6f0-4d7a-a6ec-b9ce1acc2e34",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5afd90-fdf2-4f83-bbba-a2e9b33aee8e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "5. Según el código de calibración de la cámara en la celda \"# procedimiento de calibración\". ¿Dónde está situado el sistema de coordenadas del mundo? ¿Cuáles son las coordenadas de los dos primeros puntos del damero?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77d55d-3ea9-4773-918d-c7d34e6e6f76",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d98271-8d29-49d5-b0ec-89305289a3d3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "6. ¿Cual es el cometido de la función `findChessboardCorners`? Describa el significado de los parámetros de entrada y salida.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3183f7-ea99-4d14-8362-3bc7ac0338dd",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1eaa81-09b7-43de-943c-fd89999b6394",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "7. ¿Cuál es la función encargada de realizar la calibración propiamente dicha?. Describa el significado de los parámetros de entrada y salida.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e123ce4-e0c4-4a16-8293-584b6ca82c76",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5a0d5-80a0-4401-838e-e06cdb317a44",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "8. Ejecute el código \"# procedimiento de calibración\" para calibrar la cámara cuyas imágenes están en la carpeta \"'data_calibracion/figs/calibracion/*.jpg'\". El programa marca los puntos correspondientes en el plano objeto y en el plano imagen para cada imagen adquirida. De todas las imágenes disponibles, ¿cuántas imágenes son válidas? ¿Por qué cree que el resto no son válidas?.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d691522-0341-4f15-b186-f19acefd360a",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13969194-8be0-44ab-b7c4-1069540b7fa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "9. A través de la matriz de la cámara, razone si los píxeles son cuadrados o no y la posición del punto principal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd01791-3f60-4168-942b-11605dd8e2ff",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31cbaf1-63db-420a-a956-9a5ad515b33b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "10. En esta parte, cada alumno deberá calibrar la cámara de su teléfono móvil, para ello tome diferentes fotos (unas 10 fotos suele ser suficiente) de un patrón conocido de tablero de ajedrez. Utilice la imagen del damero situada en \"data_calibracion/figs/pattern.jpg\" y muéstrela en la pantalla del portátil a una escala de 100%. A continuación tome las fotos del damero con el móvil situado en diferentes distancias y orientaciones, guarde las fotos en la carpeta \"data_calibracion/figs/calibracion_movil\" en formato \".jpg\". Finalmente ejecute el siguiente código de calibración. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80072ee-4b2a-4d48-9589-c9cc9536f070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# procedimiento de calibración\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Defining the dimensions of checkerboard\n",
    "CHECKERBOARD = (6,9)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# Creating vector to store vectors of 3D points for each checkerboard image\n",
    "objpoints = []\n",
    "# Creating vector to store vectors of 2D points for each checkerboard image\n",
    "imgpoints = [] \n",
    "\n",
    "# Defining the world coordinates for 3D points\n",
    "objp = np.zeros((1, CHECKERBOARD[0]*CHECKERBOARD[1], 3), np.float32)\n",
    "objp[0,:,:2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\n",
    "prev_img_shape = None\n",
    "\n",
    "# Extracting path of individual image stored in a given directory\n",
    "images = glob.glob('data_calibracion/figs/calibracion_movil/*.jpg')\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.resize(img, None, fx=0.25, fy=0.25, interpolation = cv2.INTER_LINEAR)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    # Find the chess board corners\n",
    "    # If desired number of corners are found in the image then ret = true\n",
    "    ret, corners = cv2.findChessboardCorners(gray, CHECKERBOARD, cv2.CALIB_CB_ADAPTIVE_THRESH+\n",
    "    \tcv2.CALIB_CB_FAST_CHECK+cv2.CALIB_CB_NORMALIZE_IMAGE)\n",
    "    \"\"\"\n",
    "    If desired number of corner are detected,\n",
    "    we refine the pixel coordinates and display \n",
    "    them on the images of checker board\n",
    "    \"\"\"\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        # refining pixel coordinates for given 2d points.\n",
    "        corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
    "        imgpoints.append(corners2)\n",
    "       # Draw and display the corners\n",
    "        img = cv2.drawChessboardCorners(img, CHECKERBOARD, corners2,ret)\n",
    "    cv2.imshow('img',img)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "h,w = img.shape[:2]\n",
    "\"\"\"\n",
    "Performing camera calibration by \n",
    "passing the value of known 3D points (objpoints)\n",
    "and corresponding pixel coordinates of the \n",
    "detected corners (imgpoints)\n",
    "\"\"\"\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)\n",
    "\n",
    "print(\"Camera matrix : \\n\")\n",
    "print(mtx)\n",
    "print(\"dist : \\n\")\n",
    "print(dist)\n",
    "print(\"rvecs : \\n\")\n",
    "print(rvecs)\n",
    "print(\"tvecs : \\n\")\n",
    "print(tvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786187e0-0ab9-418b-aeec-2ba285e7ceb5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Muestre los resultados de la calibración y explique razonadamente toda la información que se pueda deducir de la matriz de la cámara.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca2bb25-f8b2-47de-ba95-9b6846f581bf",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5302c3c-fa4c-4453-af19-d7793c207a7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "11. A continuación vamos a compensar la distorsión de la cámara en la primera de las imágenes que ha utilizado para la calibración, para ello ejecute la siguiente celda. Muestre la imagen con la distorsión compensada. Relacione los resultados obtenidos con los parámetros de distorsión de la cámara.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f111433-6d47-414f-ab50-e0411cde3412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the derived camera parameters to undistort the image\n",
    "img = cv2.imread(images[0])\n",
    "img = cv2.resize(img, None, fx=0.25, fy=0.25, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "# Refining the camera matrix using parameters obtained by calibration\n",
    "newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w,h), 0, (w,h))\n",
    "\n",
    "# Method 1 to undistort the image\n",
    "dst = cv2.undistort(img, mtx, dist, None, newcameramtx)\n",
    "# Method 2 to undistort the image\n",
    "#mapx,mapy=cv2.initUndistortRectifyMap(mtx,dist,None,newcameramtx,(w,h),5)\n",
    "#dst = cv2.remap(img,mapx,mapy,cv2.INTER_LINEAR)\n",
    "\n",
    "# Displaying the undistorted image\n",
    "cv2.imshow(\"distorted image\",img)\n",
    "cv2.imshow(\"undistorted image\",dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1817de77-bcac-4016-b089-1751bb98bbfc",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60c57e86-2a96-4d7d-bed0-dbfa76f9c2c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "12. Pregunta avanzada:\n",
    "\n",
    "Repetir el proceso de calibración y compensación de la distorsión con una webcam. Mostrar y comentar los resultados obtenidos.\n",
    "\n",
    ">Nota: las webcams suelen tener una óptica muy básica y generan fuertes distorsiones en la imagen.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9ceb4-b7eb-4011-81db-9195351c408a",
   "metadata": {},
   "source": [
    "%%%% TO DO %%%%%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pivc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5aa2ca8ce05eb79bb4150b5e8f80085d3d22a7c78b599a1efee5ea88f4c2c174"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
